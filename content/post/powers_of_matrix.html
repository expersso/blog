---
title: Taking powers of a matrix in R
author: EP
date: '2017-08-08'
categories:
  - mathematics
  - R
tags:
  - linear algebra
---



<p>Consider a number, say 4. If we want to multiply this number by itself 3 times we can write this as either <span class="math inline">\(4 \times 4 \times 4\)</span>, or, more compactly, as <span class="math inline">\(4^3\)</span>. This operation is known as <em>exponentiation</em> and follows some well-known algebraic laws and conventions. For example, for any non-zero number <span class="math inline">\(x\)</span> we have:</p>
<ul>
<li><span class="math inline">\(x^k = \overbrace{x \times x \times \dots \times x}^{k \text{ times}}\)</span></li>
<li><span class="math inline">\(x^1 = x\)</span></li>
<li><span class="math inline">\(x^0 = 1\)</span></li>
<li><span class="math inline">\(x^{-1} = \frac{1}{x}\)</span></li>
<li><span class="math inline">\(x^{-k} = (\frac{1}{x})^{k}\)</span></li>
</ul>
<p>This is all surely familiar to the reader. What may not be directly obvious, however, is that we can meaningfully define analogous operations on invertible matrices and use the same notation:</p>
<ul>
<li><span class="math inline">\(A^k = \overbrace{AA \dots A}^{k \text{ times}}\)</span>: Exponentiation by positive integer is repeated multiplication.</li>
<li><span class="math inline">\(A^1 = A\)</span>: Exponentiation by identity returns original matrix.</li>
<li><span class="math inline">\(A^0 = I\)</span>: Exponentiation by zero returns the identity matrix.</li>
<li><span class="math inline">\(A^{-1} = \text{inv}(A)\)</span>: Exponentiation by -1 returns inverse of original matrix.</li>
<li><span class="math inline">\(A^{-k} = \text{inv}(A)^k\)</span>: Exponentiation by negative integer is repeated multiplication of inverse.</li>
</ul>
<p>R does not have built-in support for these operations (e.g. for a matrix <code>A</code>, <code>A^2</code> does not multiply <code>A</code> by itself, but rather squares every entry of <code>A</code>). In this post, therefore, we will implement this functionality. In doing so, we will touch on a number of topics:</p>
<ul>
<li>Defining test cases to make clear exactly what goal we’re trying to achieve, and test if our proposed solution achieves that goal</li>
<li>Higher-order functions</li>
<li>Using <code>purrr::reduce</code> to avoid loops</li>
<li>Diagonalizing a matrix (which involves an eigendecomposition)</li>
<li>Computational complexity</li>
<li>Benchmarking code</li>
</ul>
<div id="defining-test-cases" class="section level2">
<h2>Defining test cases</h2>
<p>We start by loading some necessary packages.</p>
<pre class="r"><code>library(tidyverse)
library(assertthat)
library(microbenchmark)</code></pre>
<p>Next, we define a sample <span class="math inline">\(3 \times 3\)</span> matrix that we use throughout the post.</p>
<pre class="r"><code>set.seed(1)
n &lt;- 3
(A &lt;- matrix(sample(10, n ^ 2, TRUE), n, n))</code></pre>
<pre><code>##      [,1] [,2] [,3]
## [1,]    3   10   10
## [2,]    4    3    7
## [3,]    6    9    7</code></pre>
<p>We now define a function <code>test_properties</code> that takes as input a matrix <code>A</code> and a function <code>f</code> that takes powers of that matrix. It then applies <code>f</code> to <code>A</code> for various exponents and checks if it gives the desired result. For example, for any implementation of a power function <code>f</code> we would expect that f(A, 0) (i.e. <span class="math inline">\(A\)</span> raised to the zero power) would return the identity matrix (i.e. <code>diag(1, nrow(A))</code>).</p>
<pre class="r"><code>test_properties &lt;- function(f, A) {
  list(
    &quot;A^4 == AAAA&quot;      = are_equal(f(A,  4), A %*% A %*% A %*% A),
    &quot;A^1 == A&quot;         = are_equal(f(A,  1), A),  
    &quot;A^0 == I&quot;         = are_equal(f(A,  0), diag(1, nrow(A))),
    &quot;A^-1 == inv(A)&quot;   = are_equal(f(A, -1), solve(A)),
    &quot;A^-4 == inv(A)^4&quot; = are_equal(f(A, -4), solve(A) %^% 4)
  )
}</code></pre>
<p>Since <code>test_properties</code> is a function that takes another function as input, we would call this a <em>higher-order function</em> (or a <em>functional</em> to be more precise).</p>
</div>
<div id="strategy-1-naive-repeated-multiplication" class="section level2">
<h2>Strategy 1: Naive repeated multiplication</h2>
<p>We can try out <code>test_properties</code> on a trivial implementation of a power function. (If the reader is unfamiliar with the percentage sign-notation, that is how we define infix functions in R).</p>
<pre class="r"><code>`%^%` &lt;- function(A, k) {
  A
}</code></pre>
<p>So <code>%^%</code> takes an <code>A</code> and a <code>k</code> as input, throws away the <code>k</code> and returns the <code>A</code> unchanged. Let’s see what tests it passes.</p>
<pre class="r"><code>str(test_properties(`%^%`, A))</code></pre>
<pre><code>## List of 5
##  $ A^4 == AAAA     : logi FALSE
##  $ A^1 == A        : logi TRUE
##  $ A^0 == I        : logi FALSE
##  $ A^-1 == inv(A)  : logi FALSE
##  $ A^-4 == inv(A)^4: logi FALSE</code></pre>
<p>In order to pass all the other tests we need an implementation that actually does something. The next implementation starts by defining the identity matrix <code>I</code> of the same dimensions as <code>A</code> and initializes the matrix <code>Ak</code> to be this matrix (this is a bit roundabout, but useful for didactic purposes). It then enters a <code>while</code>-loop that decrements <code>k</code> while multiplying <code>Ak</code> by <code>A</code>. Once <code>k</code> hits zero, it returns the final matrix.</p>
<pre class="r"><code>`%^%` &lt;- function(A, k) {
  I &lt;- diag(1, nrow(A))
  Ak &lt;- I
  while(k &gt; 0) {
    Ak &lt;- Ak %*% A
    k &lt;- k - 1
  }
  Ak
}

A %*% A</code></pre>
<pre><code>##      [,1] [,2] [,3]
## [1,]  109  150  170
## [2,]   66  112  110
## [3,]   96  150  172</code></pre>
<pre class="r"><code>A %^% 2</code></pre>
<pre><code>##      [,1] [,2] [,3]
## [1,]  109  150  170
## [2,]   66  112  110
## [3,]   96  150  172</code></pre>
<p>Let’s see if this implementation is any better.</p>
<pre class="r"><code>str(test_properties(`%^%`, A))</code></pre>
<pre><code>## List of 5
##  $ A^4 == AAAA     : logi TRUE
##  $ A^1 == A        : logi TRUE
##  $ A^0 == I        : logi TRUE
##  $ A^-1 == inv(A)  : logi FALSE
##  $ A^-4 == inv(A)^4: logi FALSE</code></pre>
<p>Ok, so it works for positive, but not negative, exponents. Let’s fix that.</p>
<pre class="r"><code>`%^%` &lt;- function(A, k) {
  I &lt;- diag(1, nrow(A))
  if(k &lt; 0) {
    A &lt;- solve(A)
    k &lt;- k * -1
  }
  Ak &lt;- I
  while(k &gt; 0) {
    Ak &lt;- Ak %*% A
    k &lt;- k - 1
  }
  Ak
}</code></pre>
<p>Here we just check if <code>k</code> is negative, and if it is, replace <code>A</code> with its inverse. Everything else remains unchanged.</p>
<pre class="r"><code>str(test_properties(`%^%`, A))</code></pre>
<pre><code>## List of 5
##  $ A^4 == AAAA     : logi TRUE
##  $ A^1 == A        : logi TRUE
##  $ A^0 == I        : logi TRUE
##  $ A^-1 == inv(A)  : logi TRUE
##  $ A^-4 == inv(A)^4: logi TRUE</code></pre>
<p>It now passes all tests. Beautiful. As good functional programmers, however, we feel a bit queasy about using loops, so let’s use <code>purrr::reduce</code> to get rid of it.</p>
<pre class="r"><code>`%^%` &lt;- function(A, k) {
  I &lt;- diag(1, nrow(A))
  if(k &lt; 0) {
    A &lt;- solve(A)
    k &lt;- k * -1
  }
  reduce(seq_len(k), ~. %*% A, .init = I)
}</code></pre>
<p>The first part here is identical to the earlier function, but instead of the <code>while</code> loop we call <code>reduce</code> on a dummy vector of length <code>k</code> with the identity matrix as the starting value. The machinery of <code>reduce</code> then takes care of all the details of accumulating the matrix products.</p>
<pre class="r"><code>str(test_properties(`%^%`, A))</code></pre>
<pre><code>## List of 5
##  $ A^4 == AAAA     : logi TRUE
##  $ A^1 == A        : logi TRUE
##  $ A^0 == I        : logi TRUE
##  $ A^-1 == inv(A)  : logi TRUE
##  $ A^-4 == inv(A)^4: logi TRUE</code></pre>
<p>Indeed, the implementation with <code>reduce</code> also passes our test cases.</p>
<p>If we stop to consider how this appropach scales with larger inputs of <code>k</code>, however, we notice that we always need to do exactly <code>k</code> matrix multiplications. So the growth rate is linear in <code>k</code>.<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a> Since matrix multiplication itself is very computationally demanding – between <span class="math inline">\(\mathcal{O}(n^3)\)</span> and <span class="math inline">\(\mathcal{O}(n^{2.373})\)</span> depending on the algorithm – this approach will only work for fairly small matrices and small values of <code>k</code>. Fortunately, there’s a very clever approach that involves diagonalizing <code>A</code> which runs in much better time.</p>
</div>
<div id="strategy-2-diagonalization" class="section level2">
<h2>Strategy 2: Diagonalization</h2>
<p>It’s a remarkable fact that under the <a href="https://en.wikipedia.org/wiki/Diagonalizable_matrix">right circumstances</a>, a matrix <span class="math inline">\(A\)</span> can be decomposed as <span class="math inline">\(A = PDP^{-1}\)</span>, where the columns of <span class="math inline">\(P\)</span> are the eigenvectors of <span class="math inline">\(A\)</span> and <span class="math inline">\(D\)</span> is a diagonal matrix with the eigenvalues of <span class="math inline">\(A\)</span> along the main diagonal.</p>
<p>Consider our matrix <code>A</code>. Let’s find its eigenvectors and eigenvalues.</p>
<pre class="r"><code>eigen(A)</code></pre>
<pre><code>## eigen() decomposition
## $values
## [1] 19.317668+0.000000i -3.158834+0.253746i -3.158834-0.253746i
## 
## $vectors
##               [,1]                  [,2]                  [,3]
## [1,] -0.6483318+0i -0.8600864+0.0000000i -0.8600864+0.0000000i
## [2,] -0.4288083+0i  0.2414274-0.2544460i  0.2414274+0.2544460i
## [3,] -0.6291179+0i  0.2882856+0.2326217i  0.2882856-0.2326217i</code></pre>
<p>So <span class="math inline">\(A\)</span> has three eigenvalues (which happen to be complex numbers) and three associated eigenvectors. If what we said above about the eigendecomposition is true, we should thus be able to do the following:</p>
<pre class="r"><code>eig &lt;- eigen(A)
P &lt;- eig$vectors
P_inv &lt;- solve(P)
D &lt;- diag(eig$values)

A</code></pre>
<pre><code>##      [,1] [,2] [,3]
## [1,]    3   10   10
## [2,]    4    3    7
## [3,]    6    9    7</code></pre>
<pre class="r"><code>P %*% D %*% P_inv</code></pre>
<pre><code>##      [,1]  [,2]  [,3]
## [1,] 3-0i 10+0i 10-0i
## [2,] 4-0i  3+0i  7-0i
## [3,] 6-0i  9+0i  7-0i</code></pre>
<pre class="r"><code>Re(P %*% D %*% P_inv)</code></pre>
<pre><code>##      [,1] [,2] [,3]
## [1,]    3   10   10
## [2,]    4    3    7
## [3,]    6    9    7</code></pre>
<p>So we indeed have <span class="math inline">\(A = PDP^{-1}\)</span> (since the eigenvalues are in <span class="math inline">\(\mathbb{C}\)</span> we have to coerce them back to <span class="math inline">\(\mathbb{R}\)</span> using the <code>Re</code> function).</p>
<p>Now, not all matrices can be decomposed like this and the exact conditions for when a matrix is diagonalizable is a bit involved (we would have to figure out both the algebraic and geometric multiplicity of each eigenvalue, and check if they are all equal). However, there is a sufficient (but not necessary) condition for diagonalizability, viz. that an <span class="math inline">\(n \times n\)</span> matrix is always diagonalizable if it has <span class="math inline">\(n\)</span> distinct eigenvalues. So checking for this condition should deal with most situations. (We will ignore issues of numeric stability).</p>
<p>But how does all this help us with the problem of taking powers of a matrix?</p>
<p>Observe:</p>
<p><span class="math display">\[
A^2 = AA = PDP^{-1}PDP^{-1} = PD(P^{-1}P)DP^{-1} = PDIDP^{-1} = PDDP^{-1} = PD^2P^{-1}
\]</span></p>
<p>More generally, <span class="math inline">\(A^n = PD^nP^{-1}\)</span>. And since <span class="math inline">\(D\)</span> is diagonal, we can get <span class="math inline">\(D^n\)</span> by simply raising the elements on the diagonal of <span class="math inline">\(D\)</span> to <span class="math inline">\(n\)</span>, which is a very cheap operation. No messy repeated matrix multiplication!</p>
<div class="figure">
<img src="https://media.giphy.com/media/l41JH1Ddab4rmJQxq/giphy.gif" />

</div>
<p>Let’s write an implementation.</p>
<pre class="r"><code>`%^^%` &lt;- function(A, k) {
  eig &lt;- eigen(A)
  
  # check if A is diagonalizable
  stopifnot(length(unique(eig$values)) == nrow(A))
  
  P &lt;- eig$vectors
  D &lt;- diag(eig$values ^ k)
  Ak &lt;- P %*% D %*% solve(P)
  Re(Ak)
}</code></pre>
<p>Let’s see if works.</p>
<pre class="r"><code>str(test_properties(`%^^%`, A))</code></pre>
<pre><code>## List of 5
##  $ A^4 == AAAA     : logi TRUE
##  $ A^1 == A        : logi TRUE
##  $ A^0 == I        : logi TRUE
##  $ A^-1 == inv(A)  : logi TRUE
##  $ A^-4 == inv(A)^4: logi TRUE</code></pre>
<p>Let’s also check it for some other matrix. Let <span class="math inline">\(B\)</span> be the rotate-by-90-degrees-counterclockwise matrix. We would then expect <span class="math inline">\(B^4\)</span> to be a full 360 degree turn, which is equivalent to doing nothing (i.e. the identity matrix).</p>
<pre class="r"><code>(B &lt;- matrix(c(0, 1, -1, 0), 2))</code></pre>
<pre><code>##      [,1] [,2]
## [1,]    0   -1
## [2,]    1    0</code></pre>
<pre class="r"><code>B %^^% 4</code></pre>
<pre><code>##      [,1] [,2]
## [1,]    1    0
## [2,]    0    1</code></pre>
<pre class="r"><code>str(test_properties(`%^^%`, B))</code></pre>
<pre><code>## List of 5
##  $ A^4 == AAAA     : logi TRUE
##  $ A^1 == A        : logi TRUE
##  $ A^0 == I        : logi TRUE
##  $ A^-1 == inv(A)  : logi TRUE
##  $ A^-4 == inv(A)^4: logi TRUE</code></pre>
</div>
<div id="comparing-the-two-strategies" class="section level2">
<h2>Comparing the two strategies</h2>
<p>We would expect the run time of the first approach to be quite fast for small values of <span class="math inline">\(k\)</span>, since we don’t have to do any eigendecomposition, but to grow more or less linearly as <span class="math inline">\(k\)</span> increases. Conversely, the second approach should be slow for small values of <span class="math inline">\(k\)</span> since we have to find the eigenvalues and eigenvectors if even <span class="math inline">\(k\)</span> is very small. But once that decomposition is found, the magnitude of <span class="math inline">\(k\)</span> should have little effect on how long it takes to compute.</p>
<p>We set up a vector of <code>k</code> running from 1 to 200, and for each value we take <code>A</code> to that power. We repeat that 50 times for each value of <code>k</code> and record how long each iteration takes. Finally, we filter out some outliers that otherwise make the results difficult to visualize.</p>
<pre class="r"><code>times &lt;- 1:200 %&gt;%
  map_df(~
        microbenchmark(
          &quot;repeated multiplication&quot; = A %^% .,
          &quot;diagonalization&quot; = A %^^% .,
          times = 50
        ) %&gt;%
        as_data_frame() %&gt;%
        mutate(k = .x)
  ) %&gt;%
  filter(time &lt; quantile(time, .99))</code></pre>
<p>To figure out for what value of <span class="math inline">\(k\)</span> the diagonalization approach starts being superior, we run a simple regression and figure out where the two lines intersect.</p>
<pre class="r"><code>m1 &lt;- lm(time ~ k*expr, data = times)

df_intersect &lt;-
  data_frame(&quot;expr&quot; = &quot;diagonalization&quot;, 
             &quot;k&quot;    = -1 * coef(m1)[&quot;exprdiagonalization&quot;] / 
                           coef(m1)[&quot;k:exprdiagonalization&quot;]) %&gt;%
  mutate(time = predict(m1, .))

df_intersect</code></pre>
<pre><code>## # A tibble: 1 x 3
##              expr        k     time
##             &lt;chr&gt;    &lt;dbl&gt;    &lt;dbl&gt;
## 1 diagonalization 82.48014 220569.9</code></pre>
<p>The interpretation is that for matrices of this particular size, raising it to a power less than 82 is best done with the repeated multiplication approach, and thereafter by the diagonalization approach.</p>
<p>Finally, we plot the results.</p>
<pre class="r"><code>ggplot(times, aes(x = k, y = time / 1e3, group = expr)) +
  geom_jitter(aes(color = expr), alpha = 0.1) +
  geom_smooth(method = &quot;lm&quot;, color = &quot;black&quot;, size = 0.1) +
  geom_point(data = df_intersect) +
  geom_text(aes(label = sprintf(&quot;(%0.0f, %0.0f)&quot;, k, time)),
            data = df_intersect, vjust = 2, hjust = 0, size = 3) +
  guides(color = guide_legend(override.aes = list(alpha = 1))) +
  theme_light() +
  theme(legend.position = c(1, 0), legend.justification = c(1, 0),
        legend.background = element_rect(color = &quot;grey70&quot;)) +
  labs(y = &quot;Time (microseconds)&quot;, color = NULL,
       title = &quot;Powers of a matrix&quot;,
       subtitle = paste0(&quot;Time taken to raise a 3x3 matrix to the&quot;, 
                         &quot; kth power using different approaches&quot;))</code></pre>
<p><img src="/post/powers_of_matrix_files/figure-html/plot-1.png" width="672" /></p>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>I’m not a computer scientist, though, so I’d be happy to receive input on this if the reader disagrees.<a href="#fnref1">↩</a></p></li>
</ol>
</div>
